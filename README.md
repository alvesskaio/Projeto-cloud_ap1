# Pipeline de Processamento de Cota√ß√µes B3

Este projeto implementa um pipeline local para processar arquivos XML de cota√ß√µes da B3 (Bolsa de Valores do Brasil) usando:

- **Blob Storage local**: Azurite (emulador do Azure Blob Storage)
- **Banco de dados**: PostgreSQL local
- **Processamento**: Python com SQLAlchemy e lxml

## üìã Pr√©-requisitos

### Software necess√°rio:
- Docker e Docker Compose
- Python 3.8+
- pip (gerenciador de pacotes Python)

### Servi√ßos que ser√£o executados:
- PostgreSQL (porta 5432)
- Azurite - Azure Storage Emulator (porta 10000)
- pgAdmin (opcional, porta 8080)

## üöÄ Instala√ß√£o e Configura√ß√£o

### 1. Clone o reposit√≥rio e instale depend√™ncias Python:

```bash
cd Projeto-cloud_ap1
pip install -r requirements.txt
```

### 2. Inicie os servi√ßos com Docker Compose:

```bash
# Inicia PostgreSQL e Azurite
docker-compose up -d

# Para incluir pgAdmin (interface web):
docker-compose --profile tools up -d
```

### 3. Verifique se os servi√ßos est√£o rodando:

```bash
python main.py --check
```

## üìä Estrutura do Banco de Dados

```sql
CREATE TABLE Cotacoes (
    Id SERIAL PRIMARY KEY,
    Ativo VARCHAR(10),
    DataPregao DATE,
    Abertura DECIMAL(10,2),
    Fechamento DECIMAL(10,2),
    Volume DECIMAL(18,2)
);
```

## üîÑ Como Usar o Pipeline

### Execu√ß√£o b√°sica:
```bash
# Processa cota√ß√µes da data atual
python main.py

# Processa cota√ß√µes de uma data espec√≠fica (formato YYMMDD)
python main.py 250923

# Verifica pr√©-requisitos
python main.py --check

# Exibe ajuda
python main.py --help
```

### O pipeline executa as seguintes etapas:

1. **Extra√ß√£o**:
   - Download do arquivo ZIP da B3
   - Extra√ß√£o dos arquivos XML
   - Upload para o Blob Storage local (Azurite)

2. **Transforma√ß√£o**:
   - Leitura do XML do Blob Storage
   - Parse dos dados usando lxml
   - Extra√ß√£o de: Ativo, Data Preg√£o, Abertura, Fechamento, Volume

3. **Carga**:
   - Inser√ß√£o dos dados no PostgreSQL usando SQLAlchemy
   - Processamento em lotes para otimizar performance

## üìÅ Estrutura do Projeto

```
Projeto-cloud_ap1/
‚îú‚îÄ‚îÄ main.py              # Pipeline principal
‚îú‚îÄ‚îÄ extract.py           # M√≥dulo de extra√ß√£o (download B3)
‚îú‚îÄ‚îÄ transform_load.py    # M√≥dulo de transforma√ß√£o e carga
‚îú‚îÄ‚îÄ azure_storage.py     # M√≥dulo de integra√ß√£o com Blob Storage
‚îú‚îÄ‚îÄ database.py          # Modelos SQLAlchemy e conex√£o com PostgreSQL
‚îú‚îÄ‚îÄ helpers.py           # Fun√ß√µes auxiliares
‚îú‚îÄ‚îÄ requirements.txt     # Depend√™ncias Python
‚îú‚îÄ‚îÄ docker-compose.yml   # Configura√ß√£o dos servi√ßos Docker
‚îú‚îÄ‚îÄ init-db.sql         # Script de inicializa√ß√£o PostgreSQL
‚îî‚îÄ‚îÄ README.md           # Este arquivo
```

## üîç Monitoramento e Depura√ß√£o

### Ver logs dos containers:
```bash
docker-compose logs postgres
docker-compose logs azurite
```

### Acessar PostgreSQL diretamente:
```bash
docker exec -it cotacoes_postgres psql -U postgres -d cotacoes_b3
```

### Acessar pgAdmin (se habilitado):
- URL: http://localhost:8080
- Email: admin@cotacoes.com
- Senha: admin123

### Verificar dados inseridos:
```sql
-- No PostgreSQL
SELECT COUNT(*) FROM cotacoes;
SELECT * FROM cotacoes LIMIT 10;
SELECT ativo, COUNT(*) FROM cotacoes GROUP BY ativo ORDER BY COUNT(*) DESC LIMIT 10;
```

## ‚öôÔ∏è Configura√ß√µes

### Vari√°veis de ambiente (opcionais):

```bash
# String de conex√£o com PostgreSQL
export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/cotacoes_b3"
```

### Conex√£o Azurite:
- Endpoint: http://localhost:10000/devstoreaccount1
- Container: dados-pregao-bolsa
- Connection String: (configurada em azure_storage.py)

## üõ†Ô∏è Desenvolvimento

### Executar apenas um m√≥dulo:
```bash
# S√≥ extra√ß√£o
python extract.py

# S√≥ transforma√ß√£o e carga
python transform_load.py

# Testar conex√£o com banco
python database.py
```

### Parar todos os servi√ßos:
```bash
docker-compose down

# Para remover tamb√©m os volumes (dados):
docker-compose down -v
```

## ‚ùó Solu√ß√£o de Problemas

### Erro de conex√£o PostgreSQL:
```bash
# Verificar se o container est√° rodando
docker ps
docker-compose logs postgres
```

### Erro de conex√£o Azurite:
```bash
# Verificar se o container est√° rodando
docker ps | grep azurite
docker-compose logs azurite
```

### Erro de depend√™ncias Python:
```bash
pip install --upgrade -r requirements.txt
```

### Limpar dados e recome√ßar:
```bash
docker-compose down -v
docker-compose up -d
```

## üìà Performance

- O processamento de arquivos XML grandes (70MB+) √© otimizado usando `iterparse` para economizar mem√≥ria
- Inser√ß√µes no banco s√£o feitas em lotes para melhorar performance
- √çndices foram criados nas colunas mais consultadas (Ativo, DataPregao)

## üîí Seguran√ßa

Este projeto foi desenvolvido para execu√ß√£o **local apenas**. Para uso em produ√ß√£o, considere:

- Configurar senhas seguras para PostgreSQL
- Usar SSL/TLS para conex√µes com banco
- Implementar autentica√ß√£o adequada
- Configurar backup dos dados

## üìù Logs

O pipeline gera logs detalhados mostrando:
- Status de cada etapa
- N√∫mero de registros processados
- Erros e avisos
- Tempo de execu√ß√£o